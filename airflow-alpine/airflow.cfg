[core]
logging_level = info
airflow_home = /airflow
dags_folder = /airflow/dags
remote_logging = True
remote_log_conn_id = my_conn_S3
encrypt_s3_logs = False
logging_config_class = log_config.LOGGING_CONFIG
task_log_reader = s3.task
worker_log_server_port = 8793
executor = CeleryExecutor
sql_alchemy_conn = mysql://root:${MYSQL_ENV_MYSQL_ROOT_PASSWORD}@${MYSQL_PORT_3306_TCP_ADDR}:3306/airflow
parallelism = 32
dag_concurrency = 16
load_examples = False
max_active_runs_per_dag = 1
plugins_folder = /airflow/plugins
task_runner = BashTaskRunner
fernet_key = qo4h8QRyjXAXIrrGicXJixewan-nZ_MKEb2ON_GTpis=

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_owner = Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
smtp_host = localhost
smtp_user = airflow
smtp_port = 25
smtp_password = airflow
smtp_mail_from = airflow@airflow.com

[celery]
celery_app_name = airflow.executors.celery_executor
celeryd_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://${REDIS_PORT_6379_TCP_ADDR}:6379/1
celery_result_backend = db+mysql://root:${MYSQL_ENV_MYSQL_ROOT_PASSWORD}@${MYSQL_PORT_3306_TCP_ADDR}:3306/airflow
flower_port = 5555
default_queue = default

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
catchup_by_default = False
